)
# Fit Kaplan-Meier curves
km_fit <- survfit(Surv(time, status) ~ group, data = surv_data)
# Plot 1: Kaplan-Meier survival curves
p1 <- ggsurvplot(km_fit, data = surv_data,
pval = TRUE, pval.method = TRUE,
risk.table = TRUE,
title = "Kaplan-Meier Survival Curves",
legend.title = "Group",
legend.labs = c("Control", "Treatment"),
xlab = "Time",
ylab = "Survival Probability",
palette = c("red", "blue"),
ggtheme = theme_minimal())
# Plot 2: Hazard functions (theoretical)
hazard_data <- data.frame(
time = seq(0, 10, length.out = 100),
Control = lambda_control,
Treatment = lambda_treatment
)
p2 <- ggplot(hazard_data, aes(x = time)) +
geom_line(aes(y = Control, color = "Control"), size = 1.5) +
geom_line(aes(y = Treatment, color = "Treatment"), size = 1.5) +
labs(title = "Theoretical Hazard Functions",
x = "Time",
y = "Hazard Rate",
color = "Group") +
scale_color_manual(values = c("red", "blue")) +
annotate("text", x = 5, y = 0.18,
label = paste0("Hazard Ratio = ", hr), size = 5) +
theme_minimal()
# Plot 3: Density of survival times
p3 <- ggplot(surv_data, aes(x = time, fill = group)) +
geom_density(alpha = 0.5) +
labs(title = "Distribution of Survival Times",
x = "Time",
y = "Density",
fill = "Group") +
scale_fill_manual(values = c("red", "blue")) +
theme_minimal()
# Plot 4: Cumulative events
p4 <- ggplot(surv_data, aes(x = time, color = group)) +
stat_ecdf(size = 1.5) +
labs(title = "Cumulative Event Distribution",
x = "Time",
y = "Proportion of Events",
color = "Group") +
scale_color_manual(values = c("red", "blue")) +
theme_minimal()
# Display all plots
print(p1)
print(p2)
print(p3)
print(p4)
# Fit Cox model to verify hazard ratio
cox_fit <- coxph(Surv(time, status) ~ group, data = surv_data)
summary(cox_fit)
## Code for theoretical densities
x <- seq(0, 10, length.out = 1e4)
plot(x, dexp(x, rate = lambda_control), type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Density", main = "Theoretical Density Functions")
lines(x, dexp(x, rate = lambda_treatment), col = "blue", lwd = 2)
legend("topright", legend = c("Control", "Treatment"),
col = c("red", "blue"), lwd = 2)
plot(x, dexp(x, rate = lambda_control), type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Density", main = "Density Functions",
cex.main =2)
lines(x, dexp(x, rate = lambda_treatment), col = "blue", lwd = 2)
legend("topright", legend = c("Control", "Treatment"),
col = c("red", "blue"), lwd = 2)
x <- seq(0, 10, length.out = 1e4)
plot(x, dexp(x, rate = lambda_control), type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Density", main = "Density Functions with HR = 0.7",
cex.main =2)
lines(x, dexp(x, rate = lambda_treatment), col = "blue", lwd = 2)
legend("topright", legend = c("Control", "Treatment"),
col = c("red", "blue"), lwd = 2)
legend("topright", legend = c("Control", "Treatment"),
col = c("red", "blue"), lwd = 2,
cex = 1.5, bty = "n")
x <- seq(0, 10, length.out = 1e4)
plot(x, dexp(x, rate = lambda_control), type = "l", col = "red", lwd = 2,
xlab = "Time", ylab = "Density", main = "Density Functions with HR = 0.7",
cex.main =2)
lines(x, dexp(x, rate = lambda_treatment), col = "blue", lwd = 2)
legend("topright", legend = c("Control", "Treatment"),
col = c("red", "blue"), lwd = 2,
cex = 1.5, bty = "n")
x <- seq(0, 10, length.out = 1e4)
plot(x, dexp(x, rate = lambda_control), type = "l", col = "red", lwd = 3,
xlab = "Time", ylab = "Density", main = "Density Functions with HR = 0.7",
cex.main =2)
lines(x, dexp(x, rate = lambda_treatment), col = "blue", lwd = 3)
legend("topright", legend = c("Control", "Treatment"),
col = c("red", "blue"), lwd = 2,
cex = 1.5, bty = "n")
plot(x, dexp(x, rate = lambda_control), type = "l", col = "red", lwd = 3,
xlab = "Time", ylab = "Density", main = "Density Functions with HR = 0.7",
cex.main =2,
bty = "n")
lines(x, dexp(x, rate = lambda_treatment), col = "blue", lwd = 3)
legend("topright", legend = c("Control", "Treatment"),
col = c("red", "blue"), lwd = 2,
cex = 1.5, bty = "n")
ggsurvplot(km_fit, conf.int = TRUE,          # Show CIs
conf.int.style = "ribbon",        # Shaded band
conf.int.alpha = 0.2,             # Transparency
risk.table = TRUE)                # Add risk table
## Loading the required libraries
library(dplyr)
library(lubridate)
library(tidyr)
library(caret)
library(e1071)
library(cluster)
library(reshape2)
library(dbscan)
library(solitude)   # Isolation Forest
library(depmixS4)   # HMMs
library(igraph)
library(ggplot2)
library(nimble)     # MCMC implementation (bonus)
## Reading the data
df <- read.csv("../Data/fraudTrain.csv")
setwd("~/Course Work/SEM-6/MTH443/FraudDetectionMTH443/Code")
## Loading the required libraries
library(dplyr)
library(lubridate)
library(tidyr)
library(caret)
library(e1071)
library(cluster)
library(reshape2)
library(dbscan)
library(solitude)   # Isolation Forest
library(depmixS4)   # HMMs
library(igraph)
library(ggplot2)
library(nimble)     # MCMC implementation (bonus)
## Reading the data
df <- read.csv("../Data/fraudTrain.csv")
set.seed(123)
# Sample 10k observations with class balance
df_sample <- df %>%
sample_n(size = min(10000, n()), replace = FALSE) %>%
ungroup()
# Preprocess this sample the same way
df_sample$trans_date_trans_time <- ymd_hms(df_sample$trans_date_trans_time)
df_sample$dob <- ymd(df_sample$dob)
df_sample$age <- as.integer(time_length(difftime(df_sample$trans_date_trans_time, df_sample$dob), "years"))
df_sample$hour <- hour(df_sample$trans_date_trans_time)
df_sample$distance <- sqrt((df_sample$lat - df_sample$merch_lat)^2 + (df_sample$long - df_sample$merch_long)^2)
features <- df_sample %>%
dplyr::select(amt, age, city_pop, hour, distance) %>%
drop_na()
features_scaled <- scale(features)
## Isolation Forest
iso_model <- isolationForest$new()
iso_model$fit(df_sample[, c("amt", "age", "city_pop", "hour", "distance")])
df_sample$iso_score <- iso$predict(features)$anomaly_score
df_sample$iso_score <- iso_model$predict(features)$anomaly_score
ggplot(df_sample, aes(x = iso_score, fill = as.factor(is_fraud))) +
geom_density(alpha = 0.5) +
labs(title = "Isolation Forest Anomaly Score by Class",
x = "Anomaly Score", fill = "Is Fraud") +
theme_minimal()
## One class SVM
svm_model <- svm(features_scaled, type = "one-classification", kernel = "radial", nu = 0.05)
df_sample$svm_pred <- predict(svm_model, features_scaled)
## DBSCAN
db <- dbscan(features_scaled, eps = 1.5, minPts = 10)
df_sample$dbscan_cluster <- db$cluster
df_sample$dbscan_anomaly <- ifelse(df_sample$dbscan_cluster == 0, 1, 0)
library(reshape2)
detect_mat <- df_sample %>%
dplyr::select(is_fraud, iso_score, svm_pred, dbscan_anomaly) %>%
mutate(
iso_flag = ifelse(iso_score > 0.65, 1, 0)  # You can tune this
) %>%
dplyr::select(is_fraud, iso_flag, svm_pred, dbscan_anomaly)
detect_long <- melt(detect_mat, id.vars = "is_fraud")
ggplot(detect_long, aes(x = variable, fill = as.factor(value))) +
geom_bar(position = "fill") +
facet_wrap(~is_fraud, labeller = labeller(is_fraud = c("0" = "Not Fraud", "1" = "Fraud"))) +
labs(y = "Proportion", fill = "Flagged", title = "Model Flags by Fraud Status")
## HMMs
features <- as.data.frame(lapply(features, function(x) ifelse(is.infinite(x), NA, x)))
features <- features[complete.cases(features), ]
apply(features, 2, sd)
features_scaled <- as.data.frame(scale(features))
hmm_model <- depmix(
list(amt ~ 1, age ~ 1, city_pop ~ 1, hour ~ 1, distance ~ 1),
data = features_scaled,
nstates = 2,
family = list(gaussian(), gaussian(), gaussian(), gaussian(), gaussian())
)
set.seed(123)
hmm_fit <- fit(hmm_model)
summary(hmm_fit)
posterior_probs <- posterior(hmm_fit)
df_sample$state <- posterior_probs$state
# Assign meaningful labels to the states based on average amount (as proxy for risk)
state_summary <- df_sample %>%
group_by(state) %>%
summarise(mean_amt = mean(amt, na.rm = TRUE)) %>%
arrange(desc(mean_amt)) %>%
mutate(risk_label = c("High-Risk", "Low-Risk"))
# Merge labels back to the data
df_sample <- df_sample %>%
left_join(state_summary %>% dplyr::select(state, risk_label), by = "state")
# Plot with better labels and limits
ggplot(df_sample, aes(x = amt, fill = risk_label)) +
geom_histogram(bins = 50, position = "identity", alpha = 0.6) +
scale_fill_manual(values = c("High-Risk" = "#e74c3c", "Low-Risk" = "#2ecc71")) +
xlim(0, quantile(df_sample$amt, 0.99)) +
labs(
title = "Transaction Amount Distribution by HMM-Inferred Risk",
x = "Transaction Amount",
y = "Count",
fill = "HMM-Inferred Risk"
) +
theme_minimal(base_size = 14)
### Tree based Detection
# Sample smaller subset for speed
df_graph <- df_sample %>%
dplyr::select(cc_num, merchant, trans_date_trans_time, is_fraud) %>%
mutate(trans_date = trans_date_trans_time) %>%
arrange(trans_date) %>%
distinct()
# Build edge list: card numbers using the same merchant
edge_list <- df_graph %>%
group_by(merchant) %>%
filter(n() > 1) %>%  # Only merchants with >1 transaction
summarise(pairs = combn(cc_num, 2, simplify = FALSE)) %>%
unnest(cols = pairs) %>%
mutate(
from = sapply(pairs, `[`, 1),  # Extract first element of the pair
to = sapply(pairs, `[`, 2)     # Extract second element of the pair
) %>%
dplyr::select(from, to)
# Create the graph
g <- graph_from_data_frame(edge_list, directed = FALSE)
# Add node attributes
V(g)$fraud <- ifelse(V(g)$name %in% df_graph$cc_num[df_graph$is_fraud == 1], 1, 0)
# Detect communities
communities <- cluster_louvain(g)
V(g)$community <- communities$membership
# Visualize with fraud coloring
plot(
g,
vertex.label = NA,
vertex.size = 3,
vertex.color = ifelse(V(g)$fraud == 1, "red", "lightblue"),
main = "Graph-Based Fraud Network",
layout = layout_with_fr(g)
)
###############################################################################
#### Testing Data
###############################################################################
# Load test data
test_data <- read.csv("../Data/fraudTest.csv")
# Feature engineering (same as training set)
test_data <- test_data %>%
mutate(
trans_date_trans_time = ymd_hms(trans_date_trans_time),
hour = hour(trans_date_trans_time),
age = year(trans_date_trans_time) - year(ymd(dob)),
distance = sqrt((lat - merch_lat)^2 + (long - merch_long)^2)
) %>%
dplyr::select(amt, age, city_pop, hour, distance, is_fraud)
## Isolation Forest
iso_pred <- iso_model$predict(test_data[, c("amt", "age", "city_pop", "hour", "distance")])
# Add predictions to your test data
test_data$iso_score <- iso_pred$anomaly_score
test_data$iso_anomaly <- ifelse(test_data$iso_score > 0.65, 1, 0)  # You can change threshold
## One class SVM
test_data$svm_anomaly <- as.integer(predict(svm_model, test_data[, 1:5]) == FALSE)
## DBSCAN
db_test <- dbscan(scale(test_data[, 1:5]), eps = 0.5, minPts = 5)
test_data$dbscan_anomaly <- as.integer(db_test$cluster == 0)
## Visualisations
detect_mat <- test_data %>%
dplyr::select(is_fraud, iso_anomaly, svm_anomaly, dbscan_anomaly) %>%
mutate(across(everything(), as.factor))
melted <- melt(detect_mat, id.vars = "is_fraud")
## Prediction Comp
ggplot(melted, aes(x = value, fill = is_fraud)) +
geom_bar(position = "dodge") +
facet_wrap(~variable) +
labs(
title = "Anomaly Model Predictions vs Actual Fraud Labels",
x = "Predicted as Anomaly?",
y = "Count",
fill = "True Fraud"
) +
scale_x_discrete(labels = c("0" = "Legit", "1" = "Anomaly")) +
theme_minimal()
## Confusion Matrix
confusionMatrix(
factor(test_data$iso_anomaly),
factor(test_data$is_fraud),
positive = "1"
)
## ROC
library(pROC)
roc_iso <- roc(test_data$is_fraud, test_data$iso_anomaly)
roc_svm <- roc(test_data$is_fraud, test_data$svm_anomaly)
plot(roc_iso, col = "blue", main = "ROC Curve for Anomaly Detectors")
lines(roc_svm, col = "red")
legend("bottomright", legend = c("Isolation Forest", "One-Class SVM"), col = c("blue", "red"), lty = 1)
write.csv(test_data, "../Data/fraudTest_predictions.csv", row.names = FALSE)
features <- as.data.frame(lapply(features, function(x) ifelse(is.infinite(x), NA, x)))
features <- features[complete.cases(features), ]
apply(features, 2, sd)
features_scaled <- as.data.frame(scale(features))
hmm_model <- depmix(
list(amt ~ 1, age ~ 1, city_pop ~ 1, hour ~ 1, distance ~ 1),
data = features_scaled,
nstates = 2,
family = list(gaussian(), gaussian(), gaussian(), gaussian(), gaussian())
)
set.seed(123)
hmm_fit <- fit(hmm_model)
summary(hmm_fit)
posterior_probs <- posterior(hmm_fit)
df_sample$state <- posterior_probs$state
# Assign meaningful labels to the states based on average amount (as proxy for risk)
state_summary <- df_sample %>%
group_by(state) %>%
summarise(mean_amt = mean(amt, na.rm = TRUE)) %>%
arrange(desc(mean_amt)) %>%
mutate(risk_label = c("High-Risk", "Low-Risk"))
# Merge labels back to the data
df_sample <- df_sample %>%
left_join(state_summary %>% dplyr::select(state, risk_label), by = "state")
# Plot with better labels and limits
ggplot(df_sample, aes(x = amt, fill = risk_label)) +
geom_histogram(bins = 50, position = "identity", alpha = 0.6) +
scale_fill_manual(values = c("High-Risk" = "#e74c3c", "Low-Risk" = "#2ecc71")) +
xlim(0, quantile(df_sample$amt, 0.99)) +
labs(
title = "Transaction Amount Distribution by HMM-Inferred Risk",
x = "Transaction Amount",
y = "Count",
fill = "HMM-Inferred Risk"
) +
theme_minimal(base_size = 14)
# Sample 10k observations with class balance
df_sample <- df %>%
sample_n(size = min(10000, n()), replace = FALSE) %>%
ungroup()
# Preprocess this sample the same way
df_sample$trans_date_trans_time <- ymd_hms(df_sample$trans_date_trans_time)
df_sample$dob <- ymd(df_sample$dob)
df_sample$age <- as.integer(time_length(difftime(df_sample$trans_date_trans_time, df_sample$dob), "years"))
df_sample$hour <- hour(df_sample$trans_date_trans_time)
df_sample$distance <- sqrt((df_sample$lat - df_sample$merch_lat)^2 + (df_sample$long - df_sample$merch_long)^2)
features <- df_sample %>%
dplyr::select(amt, age, city_pop, hour, distance) %>%
drop_na()
features_scaled <- scale(features)
## Isolation Forest
iso_model <- isolationForest$new()
iso_model$fit(df_sample[, c("amt", "age", "city_pop", "hour", "distance")])
df_sample$iso_score <- iso_model$predict(features)$anomaly_score
ggplot(df_sample, aes(x = iso_score, fill = as.factor(is_fraud))) +
geom_density(alpha = 0.5) +
labs(title = "Isolation Forest Anomaly Score by Class",
x = "Anomaly Score", fill = "Is Fraud") +
theme_minimal()
## One class SVM
svm_model <- svm(features_scaled, type = "one-classification", kernel = "radial", nu = 0.5)
df_sample$svm_pred <- predict(svm_model, features_scaled)
## DBSCAN
db <- dbscan(features_scaled, eps = 1.5, minPts = 10)
df_sample$dbscan_cluster <- db$cluster
df_sample$dbscan_anomaly <- ifelse(df_sample$dbscan_cluster == 0, 1, 0)
library(reshape2)
detect_mat <- df_sample %>%
dplyr::select(is_fraud, iso_score, svm_pred, dbscan_anomaly) %>%
mutate(
iso_flag = ifelse(iso_score > 0.65, 1, 0)  # You can tune this
) %>%
dplyr::select(is_fraud, iso_flag, svm_pred, dbscan_anomaly)
detect_long <- melt(detect_mat, id.vars = "is_fraud")
ggplot(detect_long, aes(x = variable, fill = as.factor(value))) +
geom_bar(position = "fill") +
facet_wrap(~is_fraud, labeller = labeller(is_fraud = c("0" = "Not Fraud", "1" = "Fraud"))) +
labs(y = "Proportion", fill = "Flagged", title = "Model Flags by Fraud Status")
## One class SVM
svm_model <- svm(features_scaled, type = "one-classification", kernel = "radial", nu = 0.1)
df_sample$svm_pred <- predict(svm_model, features_scaled)
## DBSCAN
db <- dbscan(features_scaled, eps = 1.5, minPts = 10)
df_sample$dbscan_cluster <- db$cluster
df_sample$dbscan_anomaly <- ifelse(df_sample$dbscan_cluster == 0, 1, 0)
library(reshape2)
detect_mat <- df_sample %>%
dplyr::select(is_fraud, iso_score, svm_pred, dbscan_anomaly) %>%
mutate(
iso_flag = ifelse(iso_score > 0.65, 1, 0)  # You can tune this
) %>%
dplyr::select(is_fraud, iso_flag, svm_pred, dbscan_anomaly)
detect_long <- melt(detect_mat, id.vars = "is_fraud")
ggplot(detect_long, aes(x = variable, fill = as.factor(value))) +
geom_bar(position = "fill") +
facet_wrap(~is_fraud, labeller = labeller(is_fraud = c("0" = "Not Fraud", "1" = "Fraud"))) +
labs(y = "Proportion", fill = "Flagged", title = "Model Flags by Fraud Status")
## One class SVM
svm_model <- svm(features_scaled, type = "one-classification", kernel = "radial", nu = 1)
## One class SVM
svm_model <- svm(features_scaled, type = "one-classification", kernel = "radial", nu = 1)
## One class SVM
svm_model <- svm(features_scaled, type = "one-classification", kernel = "radial", nu = 0.8)
df_sample$svm_pred <- predict(svm_model, features_scaled)
## DBSCAN
db <- dbscan(features_scaled, eps = 1.5, minPts = 10)
df_sample$dbscan_cluster <- db$cluster
df_sample$dbscan_anomaly <- ifelse(df_sample$dbscan_cluster == 0, 1, 0)
library(reshape2)
detect_mat <- df_sample %>%
dplyr::select(is_fraud, iso_score, svm_pred, dbscan_anomaly) %>%
mutate(
iso_flag = ifelse(iso_score > 0.65, 1, 0)  # You can tune this
) %>%
dplyr::select(is_fraud, iso_flag, svm_pred, dbscan_anomaly)
detect_long <- melt(detect_mat, id.vars = "is_fraud")
ggplot(detect_long, aes(x = variable, fill = as.factor(value))) +
geom_bar(position = "fill") +
facet_wrap(~is_fraud, labeller = labeller(is_fraud = c("0" = "Not Fraud", "1" = "Fraud"))) +
labs(y = "Proportion", fill = "Flagged", title = "Model Flags by Fraud Status")
## One class SVM
svm_model <- svm(features_scaled, type = "one-classification", kernel = "radial", nu = 0.9)
df_sample$svm_pred <- predict(svm_model, features_scaled)
## DBSCAN
db <- dbscan(features_scaled, eps = 1.5, minPts = 10)
df_sample$dbscan_cluster <- db$cluster
df_sample$dbscan_anomaly <- ifelse(df_sample$dbscan_cluster == 0, 1, 0)
library(reshape2)
detect_mat <- df_sample %>%
dplyr::select(is_fraud, iso_score, svm_pred, dbscan_anomaly) %>%
mutate(
iso_flag = ifelse(iso_score > 0.65, 1, 0)  # You can tune this
) %>%
dplyr::select(is_fraud, iso_flag, svm_pred, dbscan_anomaly)
detect_long <- melt(detect_mat, id.vars = "is_fraud")
ggplot(detect_long, aes(x = variable, fill = as.factor(value))) +
geom_bar(position = "fill") +
facet_wrap(~is_fraud, labeller = labeller(is_fraud = c("0" = "Not Fraud", "1" = "Fraud"))) +
labs(y = "Proportion", fill = "Flagged", title = "Model Flags by Fraud Status")
## One class SVM
svm_model <- svm(features_scaled, type = "one-classification", kernel = "radial", nu = 0.99)
df_sample$svm_pred <- predict(svm_model, features_scaled)
## DBSCAN
db <- dbscan(features_scaled, eps = 1.5, minPts = 10)
df_sample$dbscan_cluster <- db$cluster
df_sample$dbscan_anomaly <- ifelse(df_sample$dbscan_cluster == 0, 1, 0)
library(reshape2)
detect_mat <- df_sample %>%
dplyr::select(is_fraud, iso_score, svm_pred, dbscan_anomaly) %>%
mutate(
iso_flag = ifelse(iso_score > 0.65, 1, 0)  # You can tune this
) %>%
dplyr::select(is_fraud, iso_flag, svm_pred, dbscan_anomaly)
detect_long <- melt(detect_mat, id.vars = "is_fraud")
ggplot(detect_long, aes(x = variable, fill = as.factor(value))) +
geom_bar(position = "fill") +
facet_wrap(~is_fraud, labeller = labeller(is_fraud = c("0" = "Not Fraud", "1" = "Fraud"))) +
labs(y = "Proportion", fill = "Flagged", title = "Model Flags by Fraud Status")
# Plot with better labels and limits
ggplot(df_sample, aes(x = amt, fill = risk_label)) +
geom_histogram(bins = 50, position = "identity", alpha = 0.6) +
scale_fill_manual(values = c("High-Risk" = "#e74c3c", "Low-Risk" = "#2ecc71")) +
xlim(0, quantile(df_sample$amt, 0.99)) +
labs(
title = "Transaction Amount Distribution by HMM-Inferred Risk",
x = "Transaction Amount",
y = "Count",
fill = "HMM-Inferred Risk"
) +
theme_minimal(base_size = 14)
## HMMs
features <- as.data.frame(lapply(features, function(x) ifelse(is.infinite(x), NA, x)))
features <- features[complete.cases(features), ]
apply(features, 2, sd)
features_scaled <- as.data.frame(scale(features))
hmm_model <- depmix(
list(amt ~ 1, age ~ 1, city_pop ~ 1, hour ~ 1, distance ~ 1),
data = features_scaled,
nstates = 2,
family = list(gaussian(), gaussian(), gaussian(), gaussian(), gaussian())
)
set.seed(123)
hmm_fit <- fit(hmm_model)
# Feature engineering (same as training set)
test_data <- test_data %>%
mutate(
trans_date_trans_time = ymd_hms(trans_date_trans_time),
hour = hour(trans_date_trans_time),
age = year(trans_date_trans_time) - year(ymd(dob)),
distance = sqrt((lat - merch_lat)^2 + (long - merch_long)^2)
) %>%
dplyr::select(amt, age, city_pop, hour, distance, is_fraud)
# Feature engineering (same as training set)
test_data <- test_data %>%
mutate(
trans_date_trans_time = ymd_hms(trans_date_trans_time),
hour = hour(trans_date_trans_time),
age = year(trans_date_trans_time) - year(ymd(dob)),
distance = sqrt((lat - merch_lat)^2 + (long - merch_long)^2)
) %>%
dplyr::select(amt, age, city_pop, hour, distance, is_fraud)
